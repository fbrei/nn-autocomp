{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self teaching auto completion engine\n",
    "\n",
    "## Part 1 - Predicting a single token based on the previous word\n",
    "\n",
    "In the first part of this notebook we will explore whether it is possible to train a neural network to predict one word of source code given another word, that is, toss one word at it and make it guess which one comes next. We will do this with Python source code as it is widely used and easy to learn.\n",
    "\n",
    "### Why do we do this\n",
    "\n",
    "Auto completion is such a nice feature to have in a text editor. Basically every at least moderately decent text editor is capable of doing completion based on words that you have entered before. [Vim](http://www.vim.org/) for example can even complete whole lines for you (C-X C-L).\n",
    "\n",
    "The next level of completion somewhat context based, as in most IDEs. You can enter the name of a Java object, followed by a dot and the IDE shows you a list of available methods.\n",
    "\n",
    "And then there are [snippets](https://github.com/SirVer/ultisnips). You just type a certain keyword like ```class``` and it gets expanded into a whole construct, containing all the placeholders and boilerplate that make sure that your code runs smoothly and is well documented.\n",
    "\n",
    "The last two is basically what you want and need as a programmer to avoid retyping boilerplate over and over again and to remind yourself from time to time to structure and document your code well. But these two rely on someone else doing the dirty work for you. Someone else has to implement rules and mechanisms that allow you to use these kinds of completion. Plus, all this work needs to be redone for other programming languages that may arise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the data\n",
    "\n",
    "First we will need to dowload a ton of Python code. We chose the ActiveState Git repository, as it contains a hell lot of code that is mostly high quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code directory already present\r\n"
     ]
    }
   ],
   "source": [
    "! ([ -d code ] && echo \"Code directory already present\") || git clone https://github.com/ActiveState/code.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the data\n",
    "\n",
    "Before we begin with our actual work, it is always beneficial to take a look at the data first, so you now what you will be dealing with. This is especially important in our case as we are working with a programming language, which looks way different than natural language.\n",
    "\n",
    "#### Gather all source files\n",
    "\n",
    "We will traverse the code directory and absorb every file that ends with .py, which should be a good enough indicator that this is indeed python source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4597 files loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "python_files = []\n",
    "for root, dirs, files in os.walk('./code'):\n",
    "    for file in files:\n",
    "        if file.endswith('.py'):\n",
    "            python_files += [ root + \"/\" + file ]\n",
    "\n",
    "num_files = len(python_files)\n",
    "print(str(num_files) + \" files loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wasn't so hard, right? Now that we have a list of all relevant files we will read all of them in and save the whole text as one giant string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read all files\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "blob = \"\"\n",
    "for file in python_files:\n",
    "    with io.open(file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        blob += f.read()\n",
    "\n",
    "all_words = blob.split()\n",
    "\n",
    "print(\"Read all files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "\n",
    "We will now use the Counter class to generate a list of all words and their corresponding frequency in the whole corpus. Python will do this more or less automatically for us, so we don't have to write too much source code here. After that, let's look at some of the most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('=', 91988)\n",
      "('#', 43096)\n",
      "('if', 33328)\n",
      "('def', 30766)\n",
      "('the', 29026)\n",
      "('in', 23738)\n",
      "('return', 20957)\n",
      "('for', 20362)\n",
      "('to', 14870)\n",
      "('a', 14759)\n",
      "('of', 12784)\n",
      "('==', 12583)\n",
      "('print', 12504)\n",
      "('and', 12445)\n",
      "('is', 12422)\n",
      "('+', 12064)\n",
      "('import', 10869)\n",
      "('\"\"\"', 8902)\n",
      "('not', 8452)\n",
      "('1', 7927)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_freqs = Counter(all_words)\n",
    "\n",
    "for w in word_freqs.most_common(20):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with a programming language, it is no big surprise that something as trivial as an equal sign is on top of the list. But we can also see that the corpus contains a lot of 'normal' english words. This may pose a problem later because we don't want to help the person in front of the screen to write better prose, but nice and clean source code. On the other hand, python contains a lot of english words like _for_ or _if_ as reserved key words. Of cource we wouldn't want to kick these out of our data.\n",
    "\n",
    "Fortunately, Python offers a way to get out of such situation. Take a look at this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['False', 'None', 'True', 'and', 'as', 'assert']\n"
     ]
    }
   ],
   "source": [
    "import keyword\n",
    "\n",
    "print(keyword.iskeyword('def'))\n",
    "print(keyword.kwlist[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How nice! We will use this a little bit later on. For now, let's focus on cleaning up the data by removing anything that isn't that important (think of comments for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - remove all comments, including inline ones, while preserving the source code in front of them\n",
    "\n",
    "import re\n",
    "\n",
    "comments = re.compile(r'#.*')\n",
    "\n",
    "cleaned_text = comments.sub('', blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we will the special characters dot, comma and end-of-line a special treatment so they stand out a little bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Pad dots and commas with whitespace, so things like \"a,\" and \"a ,\" are both recognized\n",
    "# as an 'a' followed by a comma\n",
    "\n",
    "commas = re.compile(r',')\n",
    "cleaned_text = commas.sub(' , ', cleaned_text)\n",
    "\n",
    "dots = re.compile(r'\\.')\n",
    "cleaned_text = dots.sub(' . ', cleaned_text)\n",
    "\n",
    "# Line breaks can also cause trouble if the next line is not indented\n",
    "\n",
    "cr = re.compile(r'\\n')\n",
    "cleaned_text = cr.sub(' <EOL> ', cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = re.compile(r'\\t')\n",
    "cleaned_text=tab.sub('',cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it's time to seperate brackets and operators from things like variable names by putting some whitespace between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done cleaning!\n"
     ]
    }
   ],
   "source": [
    "# Now just add some spaces to all kinds of brackets\n",
    "cleaned_text = re.sub(r'\\(', ' ( ', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\[', ' [ ', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\{', ' { ', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\)', ' ) ', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\]', ' ] ', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\}', ' } ', cleaned_text)\n",
    "\n",
    "# Equal signs are also special. We don't want something like \"a=b\" clobber our dictionary,\n",
    "# so we add spaces. Then we undo this transformation on the comparison operator \"==\" and some special\n",
    "# assignments\n",
    "cleaned_text = re.sub(r'=', ' = ', cleaned_text)\n",
    "cleaned_text = re.sub(r'=  =', '==', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\+ =', '\\+=', cleaned_text)\n",
    "cleaned_text = re.sub(r'- =', '-=', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\* =', '\\*=', cleaned_text)\n",
    "cleaned_text = re.sub(r'/ =', '/=', cleaned_text)\n",
    "\n",
    "# Let's do the same with all the operators\n",
    "cleaned_text = re.sub(r'\\+', ' \\+ ', cleaned_text)\n",
    "cleaned_text = re.sub(r'-', ' - ', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\*', ' \\* ', cleaned_text)\n",
    "cleaned_text = re.sub(r'/', ' / ', cleaned_text)\n",
    "\n",
    "cleaned_text = re.sub(r'\\+ =', '\\+=', cleaned_text)\n",
    "cleaned_text = re.sub(r'- =', '-=', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\* =', '\\*=', cleaned_text)\n",
    "cleaned_text = re.sub(r'/ =', '/=', cleaned_text)\n",
    "\n",
    "\n",
    "print(\"Done cleaning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the data\n",
    "\n",
    "The next step is to turn the words into numbers. Therefore we will create two dictionaries, one that translates words into numbers and another one that translates numbers back into words (for performance reasons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dicts(word_list):\n",
    "    \n",
    "    # The cleaned word list may contain tokens that only consist of whitespace,\n",
    "    # or that are padded with whitespace.\n",
    "    # Here we delete all whitespace in a token and drop it if nothing remains\n",
    "    cleaned_word_list = [ w.strip() for w in word_list if w.strip() != '' ]\n",
    "    \n",
    "    # Now we add a token that will later represent everything that we think\n",
    "    # is the name of a variable, constant, or any other kind of identifier\n",
    "    cleaned_word_list += ['<ID>']\n",
    "    \n",
    "    # Now we make all tokens unique. This is necessary to ensure that the values\n",
    "    # are uninterrupted\n",
    "    cleaned_word_list = set(cleaned_word_list)\n",
    "    \n",
    "    # Now we will create the tuples that consist of the future\n",
    "    # key value pairs. We have to cast this into a list because\n",
    "    # the enumerate object won't return anything after its first\n",
    "    # usage\n",
    "    word_idx_pairs = list(enumerate(cleaned_word_list))\n",
    "\n",
    "    # Dictionary comprehension. This is now trivial thanks to our\n",
    "    # work before\n",
    "    w2n_dict = { w: i for i, w in word_idx_pairs}\n",
    "    n2w_dict = { i: w for i, w in word_idx_pairs}\n",
    "\n",
    "    return w2n_dict, n2w_dict\n",
    "\n",
    "all_words = cleaned_text.split(\" \")\n",
    "w2n_dict, n2w_dict = create_dicts(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2w_dict[w2n_dict[\".\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the lookup is working in both directions. Let's see how big our dictionary has gotten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151441\n"
     ]
    }
   ],
   "source": [
    "print(len(w2n_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still a bit much. Let's see what's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nXse9q59LrUlnzHSWOOW', 't7SnuqUou', 'signatures:', \"instance>'\", 'printProgramStatus', 'POSIX', 'NYcZd9cdfdh\\\\', '\"abcdefghijklmnopqrstuvwxyzâãäåæçèéêëìíîïðñòóôõöøùúûüýþ\"', \"'0b'\", 'crtcltemp', '\"HM41e7ky3OjxzxfpJT5XPoP0nyd0eKH12Db6vT8H7', 'parser', '\"ClusterName\"', 'cXoubFQtQ', 'scale_pitch', 'vurl', 'ShTC6ViX23sSWPqHyMOtkvOZ4Ze1bxTn1\"', 'ends_with', '\"3xuVr', 'pkX1zM', 'getPath', \"'''<html><head><style>'''\", 'b0', 'ypproweMbTnHl2', 'htp', '\"matching\">', 'RUNNING:', 'Sub', '__EOF', 'd1H', '\"LzP', '\"Info\"', 'kKEXzM6P', 'set_server_documentation', 'jZr', \"div>\\\\n'\", 'Intended', 'reader_acquire', \"'<head><title>\", '<index', 'new_paragraphs:', 'fof\"', 'getRecordset', '_tracer', 'func_end', \"'s:'\", 'delete_PhotoImage', 'qMePrPQw2eLPfyn3cPrDhaww1zA\"', '\\\\xa7\\\\xcb\\\\xe3\\\\xb7\\\\xa8\\\\xa3\\\\xba\\\\n\\\\n', 'i;']\n"
     ]
    }
   ],
   "source": [
    "print(list(w2n_dict.keys())[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes, what a mess this still is!\n",
    "\n",
    "One thing that is commonly used in NLP (natural language processing) is cutting of the lower end of your word distribution. That is, take all the words that occur less then a certain threshold and just throw them overboard. Let's do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 151441\n",
      "After:  214\n"
     ]
    }
   ],
   "source": [
    "# The word count should be at least a certain fraction of the total\n",
    "# number of documents\n",
    "min_fraction = 0.2\n",
    "min_count = num_files * min_fraction\n",
    "\n",
    "# Cut off words that are simply too rare\n",
    "all_words = cleaned_text.split(\" \")\n",
    "c = Counter(all_words)\n",
    "\n",
    "reduced_words = [ w for w in all_words if c[w] >= min_count ]\n",
    "\n",
    "reduced_w2n_dict, reduced_n2w_dict = create_dicts(reduced_words)\n",
    "\n",
    "print(\"Before: \" + str(len(w2n_dict)))\n",
    "print(\"After:  \" + str(len(reduced_w2n_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the data ready for training\n",
    "\n",
    "Ok so now that we have greatly reduced our dictionary size, we still have one problem to solve. The words that were filtered out are actually still inside the text and appear alongside the words that we are interested in. So when creating the input label pairs, what should we do?\n",
    "\n",
    "In our case, we will just silently ignore these cases and return nothing. So let's get to it.\n",
    "\n",
    "First we will transform every line into a series of IDs, masking all words that we don't care about with a -1. We will then say, that all these '-1's are probably some sort of identifier for a constant or variable and will call it just ```<ID>``` later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44, 95, 44, 71, 71, 44, 44, 212, 44, 71, 71, 44, 71, 44, 44, 68, 207, 20, 71, 44, 68, 120, 20, 71, 44, 68, 120, 20, 53, 71, 71, 192, 71, 44, 44, 71, 44, 71, 44], [], [46, 46, 44, 44, 44, 69, 44, 44, 44, 46, 46], [], [44, 95, 44, 71, 71, 44, 44, 212, 44, 71, 71, 44, 71, 44, 44, 68, 207, 20, 71, 44, 68, 120, 20, 71, 44, 68, 120, 20, 53, 71, 71, 192, 71, 44, 44, 71, 44, 71, 44, 71, 71, 44, 71, 44], [], [46, 46, 44, 44, 15, 44, 44, 151, 44, 71, 44, 44, 46, 46], [], [44, 95, 44, 71, 5, 44, 212, 44, 71, 122, 44, 68, 207, 20, 71, 44, 68, 120, 20, 71, 44, 68, 120, 20, 53, 71, 186, 44, 71, 44, 71, 44, 71, 141], []]\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_num(sent, ldict):\n",
    "    transformed = []\n",
    "    for w in sent.split(\" \"):\n",
    "        if w in ldict.keys():\n",
    "            transformed.append(ldict[w])\n",
    "        elif w.strip() == \"\" :\n",
    "            continue\n",
    "        else:\n",
    "            transformed.append(ldict['<ID>'])\n",
    "    return transformed\n",
    "\n",
    "lines = [ sentence_to_num(s, reduced_w2n_dict) for s in cleaned_text.split(\"<EOL>\") ]\n",
    "\n",
    "print(lines[20:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we got all sentences tokenized and words that are irrelevant marked as such. Now we can start turning these into input target pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " def find_replace ( cfg ) : \n",
      "95\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_text.split(\"<EOL>\")[73])\n",
    "print(reduced_w2n_dict[\".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', '<ID>', '(', '<ID>', ')', ':']\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "n = 30\n",
    "for line in lines[30:100]:\n",
    "    if reduced_w2n_dict['def'] in line:\n",
    "        sent = [ reduced_n2w_dict[idx] for idx in line ]\n",
    "        print(sent)\n",
    "        print(n)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_samples(sentence, window_size=1):\n",
    "    \n",
    "    # ls stands for labeled sentence (meaning that the\n",
    "    # end of the sentence is marked with an <EOL>)\n",
    "    ls = sentence + [ reduced_w2n_dict['<EOL>'] ]\n",
    "    \n",
    "    x_vals = [ ls[i:i+window_size] for i in range(len(ls) - window_size) ]\n",
    "    y_vals = [ ls[i+window_size] for i in range(len(ls) - window_size) ]\n",
    "        \n",
    "    return x_vals, y_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "for line in lines:\n",
    "    x, y = sent_to_samples(line)\n",
    "    inputs += x\n",
    "    labels += y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[44], [44], [154], [154], [154], [154], [154], [154], [44], [44]]\n",
      "[44, 190, 154, 154, 154, 154, 154, 190, 44, 190]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[20:30])\n",
    "print(labels[20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 3079533 out of 3079533 samples\n"
     ]
    }
   ],
   "source": [
    "# If you run out of memory, set this to a value between 0 and 1 to reduce the number of samples\n",
    "keep_prob = 1.0\n",
    "\n",
    "import random\n",
    "\n",
    "# If the training data has been assigned before, delete it to\n",
    "# free memory. If we just reassign it, we end up with a lot of\n",
    "# wasted space and we won't be able to store the data in RAM any more\n",
    "try:\n",
    "    del training_inputs\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del training_labels\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "training_inputs = []\n",
    "training_labels = []\n",
    "\n",
    "for idx in range(len(inputs)):\n",
    "    if random.random() < keep_prob:\n",
    "        training_inputs.append(inputs[idx])\n",
    "        training_labels.append(labels[idx])\n",
    "        \n",
    "print(\"Kept {} out of {} samples\".format(len(training_inputs), len(inputs)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all this madness into a handy little function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_data(cleaned_text, w2n, sampling_rate = 1.0, window_size=1):\n",
    "    \n",
    "    lines = [ sentence_to_num(s, w2n) for s in cleaned_text.split(\"<EOL>\") ]\n",
    "    \n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for line in lines:\n",
    "        x, y = sent_to_samples(line, window_size=window_size)\n",
    "        inputs += x\n",
    "        labels += y\n",
    "        \n",
    "    training_inputs = []\n",
    "    training_labels = []\n",
    "\n",
    "    for idx in range(len(inputs)):\n",
    "        if random.random() < sampling_rate:\n",
    "            training_inputs.append(inputs[idx])\n",
    "            training_labels.append(labels[idx])\n",
    "            \n",
    "    training_inputs = np.array(training_inputs)\n",
    "    training_labels = to_categorical(training_labels)\n",
    "    \n",
    "    return training_inputs, training_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to introduce the neural network\n",
    "\n",
    "Now that we got our data set up, we can cast it into a numpy array and one-hot-encode the labels. This will take about 30G of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aind2/anaconda3/envs/ml/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "training_inputs = np.array(training_inputs)\n",
    "training_labels = to_categorical(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set some parameters for our network that will influence its performance. Some good default values have been already chosen for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "embedding_dim = 128\n",
    "mem_size = 128 # Number of recurrent cells (GRU in our case)\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 256\n",
    "validation_split = 0.1\n",
    "\n",
    "# ----\n",
    "load_pretrained = False\n",
    "pretrained_date = \"2018-02-20--22-21-59\"\n",
    "\n",
    "# --- Auto computed\n",
    "vocab_size = len(reduced_w2n_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is pretty much boilerplate for our case, so there is no need to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 128)            27392     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 128)               98688     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 214)               27606     \n",
      "=================================================================\n",
      "Total params: 153,686\n",
      "Trainable params: 153,686\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2771579 samples, validate on 307954 samples\n",
      "Epoch 1/3\n",
      "2771579/2771579 [==============================] - 91s 33us/step - loss: 2.6155 - val_loss: 2.5349\n",
      "Epoch 2/3\n",
      "2771579/2771579 [==============================] - 86s 31us/step - loss: 2.3631 - val_loss: 2.5034\n",
      "Epoch 3/3\n",
      "2771579/2771579 [==============================] - 86s 31us/step - loss: 2.3411 - val_loss: 2.4946\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Embedding\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "if load_pretrained:\n",
    "    print(\"Loading pretrained model instead of starting over\")\n",
    "    from keras.models import load_model\n",
    "    \n",
    "    model_file = pretrained_date + \"--model.h5\"\n",
    "    w2n_file = pretrained_date + \"--w2n.dict\"\n",
    "    n2w_file = pretrained_date + \"--n2w.dict\"\n",
    "    \n",
    "    print(\"File: \" + model_file)\n",
    "    model = load_model(model_file)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    print(\"Reshaping the training data...\")\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(w2n_file, 'rb') as f:\n",
    "        reduced_w2n_dict = pickle.load(f)\n",
    "            \n",
    "    with open(n2w_file, 'rb') as f:\n",
    "        reduced_n2w_dict = pickle.load(f)\n",
    "\n",
    "    try:\n",
    "        del training_inputs\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        del training_labels\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    training_inputs, training_labels = make_training_data(cleaned_text, reduced_w2n_dict)\n",
    "    \n",
    "    print(\"Done\")\n",
    "    \n",
    "else:\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add( Embedding(vocab_size, embedding_dim, input_length=1) )\n",
    "    model.add( GRU(mem_size) )\n",
    "    model.add( Dense(vocab_size, activation='softmax') )\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy')\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(x = training_inputs, y = training_labels, epochs = epochs, \\\n",
    "          batch_size = batch_size, verbose = 1, validation_split = validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the network\n",
    "\n",
    "Let's throw some words at the network and see what it predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if -> <ID>\n",
      "if -> not\n",
      "if -> self\n",
      "if -> __name__\n",
      "if -> len\n",
      "import -> <ID>\n",
      "import -> sys\n",
      "import -> os\n",
      "import -> time\n",
      "import -> random\n",
      "def -> <ID>\n",
      "def -> __init__\n",
      "def -> main\n",
      "def -> get\n",
      "def -> test\n",
      "( -> <ID>\n",
      "( -> self\n",
      "( -> )\n",
      "( -> (\n",
      "( -> 0\n"
     ]
    }
   ],
   "source": [
    "test_words = ['if', 'import', 'def', '(']\n",
    "top_n = 5\n",
    "\n",
    "for word in test_words:\n",
    "    test_int = reduced_w2n_dict[word]\n",
    "    \n",
    "    output = model.predict(np.array([test_int]))\n",
    "    \n",
    "    candidates = output.argsort()[0][::-1]\n",
    "    \n",
    "    for idx in range(top_n):\n",
    "        print(word, \"->\", reduced_n2w_dict[candidates[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good!\n",
    "\n",
    "Now we should save the model to reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved with timestamp 2018-02-23--21-41-22\n"
     ]
    }
   ],
   "source": [
    "if not load_pretrained:\n",
    "    import time\n",
    "    import pickle\n",
    "\n",
    "    timestr = time.strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "    filename = timestr + \"--model.h5\"\n",
    "    model.save(filename)\n",
    "\n",
    "    with open(timestr + \"--w2n.dict\", 'wb') as f:\n",
    "        pickle.dump(reduced_w2n_dict, f)\n",
    "    \n",
    "    with open(timestr + \"--n2w.dict\", 'wb') as f:\n",
    "        pickle.dump(reduced_n2w_dict, f)\n",
    "    \n",
    "    print(\"Saved with timestamp \" + timestr)\n",
    "else:\n",
    "    print(\"Was already loaded from a file, so not saving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Making longer predictions\n",
    "\n",
    "We are honestly a little bit surprised how well the first test turned out. The goal in this section is now slightly more advanced:\n",
    "\n",
    "- Train the network on bigrams to let it learn about context\n",
    "- Make predictions about whole lines, that is, until an ```<EOL>``` is predicted\n",
    "- Reuse as much from the previous part as possible\n",
    "\n",
    "So let's get straight to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del training_inputs\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    del training_labels\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs, training_labels = make_training_data(cleaned_text, reduced_w2n_dict, window_size = window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ngram = Sequential()\n",
    "\n",
    "model_ngram.add( Embedding(vocab_size, embedding_dim, input_length=window_size) )\n",
    "model_ngram.add( GRU(mem_size) )\n",
    "model_ngram.add( Dense(vocab_size, activation='softmax') )\n",
    "\n",
    "model_ngram.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2420290 samples, validate on 268922 samples\n",
      "Epoch 1/4\n",
      "2420290/2420290 [==============================] - 82s 34us/step - loss: 2.3974 - val_loss: 2.2585\n",
      "Epoch 2/4\n",
      "2420290/2420290 [==============================] - 81s 34us/step - loss: 2.0413 - val_loss: 2.1781\n",
      "Epoch 3/4\n",
      "2420290/2420290 [==============================] - 81s 34us/step - loss: 1.9906 - val_loss: 2.1429\n",
      "Epoch 4/4\n",
      "2420290/2420290 [==============================] - 81s 34us/step - loss: 1.9651 - val_loss: 2.1234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fef7a300208>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ngram.fit(x=training_inputs, y=training_labels, epochs=epochs + 1, \\\n",
    "                batch_size=batch_size, validation_split=validation_split, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', '<ID>'] -> <EOL>\n",
      "['if', '<ID>'] -> ==\n",
      "['if', '<ID>'] -> .\n",
      "['if', '<ID>'] -> [\n",
      "['def', '<ID>'] -> (\n",
      "['def', '<ID>'] -> <EOL>\n",
      "['def', '<ID>'] -> .\n",
      "['def', '<ID>'] -> [\n",
      "['main', '('] -> )\n",
      "['main', '('] -> <ID>\n",
      "['main', '('] -> self\n",
      "['main', '('] -> \\*\n",
      "['(', '<ID>'] -> )\n",
      "['(', '<ID>'] -> ,\n",
      "['(', '<ID>'] -> .\n",
      "['(', '<ID>'] -> (\n"
     ]
    }
   ],
   "source": [
    "test_words = [ ['if', '<ID>'], ['def', '<ID>'], ['main', '('] , ['(', '<ID>']]\n",
    "top_n = 4\n",
    "\n",
    "for word in test_words:\n",
    "    test_input = [ [reduced_w2n_dict[word[0]], reduced_w2n_dict[word[1]]] ]\n",
    "    \n",
    "    output = model_ngram.predict(np.array(test_input))\n",
    "    \n",
    "    candidates = output.argsort()[0][::-1]\n",
    "    \n",
    "    for idx in range(top_n):\n",
    "        print(word, \"->\", reduced_n2w_dict[candidates[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', '<ID>', '(', '<ID>', ')', '<EOL>']\n",
      "['def', '<ID>', '(', ')', '<EOL>']\n",
      "['def', '<ID>', '<EOL>', '<ID>']\n",
      "['def', '<ID>', '<EOL>']\n",
      "['def', '<ID>', '(', '<ID>', ',', '<ID>', ',', '<ID>']\n",
      "(\n",
      "<ID>\n",
      ")\n",
      "<EOL>\n"
     ]
    }
   ],
   "source": [
    "test_start = ['def', '<ID>']\n",
    "pred_steps = 4\n",
    "\n",
    "test_input = [ [reduced_w2n_dict[test_start[0]], reduced_w2n_dict[test_start[1]]] ]\n",
    "\n",
    "output = model_ngram.predict(np.array(test_input))\n",
    "candidates = output.argsort()[0][::-1]\n",
    "\n",
    "def expand_list(word_list, spread):\n",
    "    inputs = word_list[-2:]\n",
    "    real_inputs = [ [reduced_w2n_dict[inputs[0]], reduced_w2n_dict[inputs[1]]] ]\n",
    "    output = model_ngram.predict(np.array(real_inputs))\n",
    "    candidates = output.argsort()[0][::-1]\n",
    "    \n",
    "    result = [ word_list + [reduced_n2w_dict[candidates[idx]]] for idx in range(spread) ]\n",
    "    return result\n",
    "        \n",
    "n1, n3 = expand_list(test_start,2)\n",
    "n1, n2 = expand_list(n1, 2)\n",
    "n3, n4 = expand_list(n3, 2)\n",
    "n1, n5 = expand_list(n1,2)\n",
    "n1 = expand_list(n1,1)[0]\n",
    "n2 = expand_list(n2,1)[0]\n",
    "n5 = expand_list(n5,1)[0]\n",
    "n5 = expand_list(n5,1)[0]\n",
    "n5 = expand_list(n5,1)[0]\n",
    "print(n1)\n",
    "print(n2)\n",
    "print(n3)\n",
    "print(n4[:-1])\n",
    "print(n5)\n",
    "\n",
    "for i in range(pred_steps):\n",
    "    output = model_ngram.predict(np.array(test_input))\n",
    "    best_guess_idx = np.argmax(output)\n",
    "    print(reduced_n2w_dict[best_guess_idx])\n",
    "    test_input[0] = [ test_input[0][1], best_guess_idx ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved with timestamp 2018-02-23--21-55-30\n"
     ]
    }
   ],
   "source": [
    "if not load_pretrained:\n",
    "    import time\n",
    "    import pickle\n",
    "\n",
    "    timestr = time.strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "    filename = timestr + \"--model-2.h5\"\n",
    "    model_ngram.save(filename)\n",
    "    \n",
    "    print(\"Saved with timestamp \" + timestr)\n",
    "else:\n",
    "    print(\"Was already loaded from a file, so not saving.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
