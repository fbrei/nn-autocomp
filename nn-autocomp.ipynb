{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project for DH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the data\n",
    "\n",
    "First we will need to dowload a ton of Python code. We chose the ActiveState repository, as it contains a hell lot of code that is mostly high quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'code'...\n",
      "remote: Counting objects: 68352, done.\u001b[K\n",
      "remote: Total 68352 (delta 0), reused 0 (delta 0), pack-reused 68352\u001b[K\n",
      "Receiving objects: 100% (68352/68352), 12.05 MiB | 22.68 MiB/s, done.\n",
      "Resolving deltas: 100% (24509/24509), done.\n",
      "Checking connectivity... done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/ActiveState/code.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the data\n",
    "\n",
    "Before we begin with our actual work, it is always beneficial to take a look at the data first, so you now what you will be dealing with. This is especially important in our case as we are working with a programming language, which looks way different than natural language.\n",
    "\n",
    "### Gather all source files\n",
    "\n",
    "We will traverse the code directory and absorb every file that ends with .py, which should be a good enough indicator that this is indeed python source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4597 files loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "python_files = []\n",
    "for root, dirs, files in os.walk('./code'):\n",
    "    for file in files:\n",
    "        if file.endswith('.py'):\n",
    "            python_files += [ root + \"/\" + file ]\n",
    "\n",
    "print(str(len(python_files)) + \" files loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wasn't so hard, right? Now that we have a list of all relevant files we will read all of them in and save the whole text as one giant string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read all files\n"
     ]
    }
   ],
   "source": [
    "blob = \"\"\n",
    "for file in python_files:\n",
    "    with open(file) as f:\n",
    "        blob += f.read()\n",
    "\n",
    "all_words = blob.split()\n",
    "\n",
    "print(\"Read all files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "\n",
    "We will now use the Counter class to generate a list of all words and their corresponding frequency in the whole corpus. Python will do this more or less automatically for us, so we don't have to write too much source code here. After that, let's look at some of the most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('=', 91988)\n",
      "('#', 43096)\n",
      "('if', 33328)\n",
      "('def', 30766)\n",
      "('the', 29026)\n",
      "('in', 23738)\n",
      "('return', 20957)\n",
      "('for', 20362)\n",
      "('to', 14870)\n",
      "('a', 14759)\n",
      "('of', 12784)\n",
      "('==', 12583)\n",
      "('print', 12504)\n",
      "('and', 12445)\n",
      "('is', 12422)\n",
      "('+', 12064)\n",
      "('import', 10869)\n",
      "('\"\"\"', 8902)\n",
      "('not', 8452)\n",
      "('1', 7927)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_freqs = Counter(all_words)\n",
    "\n",
    "for w in word_freqs.most_common(20):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with a programming language, it is no big surprise that something as trivial as an equal sign is on top of the list. But we can also see that the corpus contains a lot of 'normal' english words. This may pose a problem later because we don't want to help the person in front of the screen to write better prose, but nice and clean source code. On the other hand, python contains a lot of english words like _for_ or _if_ as reserved key words. Of cource we wouldn't want to kick these out of our data.\n",
    "\n",
    "Fortunately, Python offers a way to get out of such situation. Take a look at this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['False', 'None', 'True', 'and', 'as', 'assert']\n"
     ]
    }
   ],
   "source": [
    "import keyword\n",
    "\n",
    "print(keyword.iskeyword('def'))\n",
    "print(keyword.kwlist[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How nice! We will use this a little bit later on. For now, let's focus on cleaning up the data by removing anything that isn't that important (think of comments for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1 - remove all comments, including inline ones, while preserving the source code in front of them\n",
    "\n",
    "import re\n",
    "\n",
    "comments = re.compile(r'#.*')\n",
    "\n",
    "cleaned_text = comments.sub('', blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we will the special characters dot, comma and end-of-line a special treatment so they stand out a little bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2 - Replace dots and commas by special symbols, so things like \"a,\" and \"a ,\" are both recognized\n",
    "# as an 'a' followed by a comma\n",
    "\n",
    "commas = re.compile(r',')\n",
    "cleaned_text = commas.sub(' <COMMA> ', cleaned_text)\n",
    "\n",
    "dots = re.compile(r'\\.')\n",
    "cleaned_text = dots.sub(' <DOT> ', cleaned_text)\n",
    "\n",
    "# Line breaks can also cause trouble if the next line is not indented\n",
    "\n",
    "cr = re.compile(r'\\n')\n",
    "cleaned_text = cr.sub(' <EOL> ', cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it's time to seperate brackets and operators from things like variable names by putting some whitespace between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done cleaning!\n"
     ]
    }
   ],
   "source": [
    "# Now just add some spaces to all kinds of brackets\n",
    "cleaned_text = re.sub(r'\\(', ' ( ', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\[', ' [ ', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\{', ' { ', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\)', ' ) ', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\]', ' ] ', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\}', ' } ', cleaned_text)\n",
    "\n",
    "# Equal signs are also special. We don't want something like \"a=b\" clobber our dictionary,\n",
    "# so we add spaces. Then we undo this transformation on the comparison operator \"==\" and some special\n",
    "# assignments\n",
    "cleaned_text = re.sub(r'=', ' = ', cleaned_text)\n",
    "cleaned_text = re.sub(r'=  =', '==', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\+ =', '\\+=', cleaned_text)\n",
    "cleaned_text = re.sub(r'- =', '-=', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\* =', '\\*=', cleaned_text)\n",
    "cleaned_text = re.sub(r'/ =', '/=', cleaned_text)\n",
    "\n",
    "# Let's do the same with all the operators\n",
    "cleaned_text = re.sub(r'\\+', ' \\+ ', cleaned_text)\n",
    "cleaned_text = re.sub(r'-', ' - ', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\*', ' \\* ', cleaned_text)\n",
    "cleaned_text = re.sub(r'/', ' / ', cleaned_text)\n",
    "\n",
    "cleaned_text = re.sub(r'\\+ =', '\\+=', cleaned_text)\n",
    "cleaned_text = re.sub(r'- =', '-=', cleaned_text)\n",
    "cleaned_text = re.sub(r'\\* =', '\\*=', cleaned_text)\n",
    "cleaned_text = re.sub(r'/ =', '/=', cleaned_text)\n",
    "\n",
    "\n",
    "print(\"Done cleaning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data\n",
    "\n",
    "The next step is to turn the words into numbers. Therefore we will create two dictionaries, one that translates words into numbers and another one that translates numbers back into words (for performance reasons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dicts(word_list):\n",
    "    w2n_dict = { }\n",
    "    n2w_dict = { }\n",
    "    \n",
    "    # We want certain things to always be in our dictionary, no matter what\n",
    "    keep_list = keyword.kwlist + ['<DOT>', '<COMMA>', '<EOL>']\n",
    "    cleaned_word_list = [ w.strip() for w in word_list ]\n",
    "    \n",
    "    full_word_list = list(set(cleaned_word_list + keep_list))\n",
    "    for i,w in enumerate(full_word_list, len(w2n_dict)):\n",
    "        w2n_dict[w] = i\n",
    "        n2w_dict[i] = w\n",
    "    return w2n_dict, n2w_dict\n",
    "\n",
    "all_words = cleaned_text.split(\" \")\n",
    "w2n_dict, n2w_dict = create_dicts(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'return'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2w_dict[w2n_dict['return']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the lookup is working in both directions. Let's see how big our dictionary has gotten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151441\n"
     ]
    }
   ],
   "source": [
    "print(len(w2n_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still a bit much. Let's see what's inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'simultaneously', 'typechecking', \"'22'\", 'FSList', 'od6WRehbe55W86jMV9\"', '6OqqUbfd\\\\', 'call;', 'LruDict', '\"sum\"', \"'SPSS_FMT_RBHEX'\", 'items\"%cnt', 'outparams', '\"v8DT3Eb1LFFSGxolbeFRdl1Gc161X5WPO8qMQyYmiYsSfwzPsVFSQnxQmRwlAwlRUp8YZf\"', '\"write\"', 'openchar:', 'getRows', 'OwnerId', '7116948092557', 'end_of_line', '\"vBy8HgG4', '\"Unpickleable', \"'NOFILE'\", 'complex1:bar', '\"mechanize', \"'fB'\", '3NO5', \"'''it\", 'sgi9cVGc45pKDpmf0qJY7NNSM', 'Rendering\"', '2014\"', 'Qs', 'qmHFlvneAe', '__slot__', 'tickers', '``cls``', '\"price\"', 'TarFileCompat', 'TAOUP', \"'convolve\", \"'_utilizes_abilities'\", 'StreamResult', 'PNG\"', 'playerIdentities:', 'Convert', 'manager\"', 'ft&0xFFFFFFFFL', \"'u3411'\", 'PyNumber_InPlaceAnd', 'freevars']\n"
     ]
    }
   ],
   "source": [
    "print(list(w2n_dict.keys())[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes, what a mess this still is!\n",
    "\n",
    "One thing that is commonly used in NLP (natural language processing) is cutting of the lower end of your word distribution. That is, take all the words that occur less then a certain threshold and just throw them overboard. Let's do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 151441\n",
      "After:  2791\n"
     ]
    }
   ],
   "source": [
    "# How often should a word appear to escape it's doom\n",
    "min_count = 50\n",
    "\n",
    "# Cut off words that are simply too rare\n",
    "all_words = cleaned_text.split(\" \")\n",
    "c = Counter(all_words)\n",
    "\n",
    "reduced_words = [ w for w in all_words if c[w] >= min_count ]\n",
    "\n",
    "reduced_w2n_dict, reduced_n2w_dict = create_dicts(reduced_words)\n",
    "\n",
    "print(\"Before: \" + str(len(w2n_dict)))\n",
    "print(\"After:  \" + str(len(reduced_w2n_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the data ready for training\n",
    "\n",
    "Ok so now that we have greatly reduced our dictionary size, we still have one problem to solve. The words that were filtered out are actually still inside the text and appear alongside the words that we are interested in. So when creating the input label pairs, what should we do?\n",
    "\n",
    "In our case, we will just silently ignore these cases and return nothing. So let's get to it.\n",
    "\n",
    "First we will transform every line into a series of IDs, masking all words that we don't care about with a -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, -1, 2627, 996, 0, 1506, 0, 1506, 2739, 2100, 623, -1, 0, 1506, 0, 1506, 839, 1506, 1257, -1, 2481, 2723, 966, 0, 1506, 1890, 2481, 878, 966, 0, 1506, 1890, 2481, 878, 966, 1693, 0, 1506, 0, 1506, 2569, 1506, 1257, -1, 1506, -1, 1506, -1, 0], [0, 0, 0], [0, 0, 1558, 0, 1558, 2550, 1444, 2670, 1745, 2032, 396, 1270, 1558, 0, 1558, 0, 0], [0, 0, 0], [0, 0, 0, 0, 0, -1, 2627, 996, 0, 1506, 0, 1506, 2739, 2100, 623, -1, 0, 1506, 0, 1506, 839, 1506, 1257, -1, 2481, 2723, 966, 0, 1506, 1890, 2481, 878, 966, 0, 1506, 1890, 2481, 878, 966, 1693, 0, 1506, 0, 1506, 2569, 1506, 1257, -1, 1506, -1, 1506, -1, 0, 1506, 0, 1506, 2032, 1506, 396, 0], [0, 0, 0], [0, 0, 1558, 0, 1558, -1, 475, 2551, 219, 302, 680, 1872, 1506, 715, -1, 1558, 0, 1558, 0, 0], [0, 0, 0], [0, 0, 0, 0, 0, -1, 2627, 996, 0, 1506, 1066, 2100, 623, -1, 0, 1506, 648, -1, 2481, 2723, 966, 0, 1506, 1890, 2481, 878, 966, 0, 1506, 1890, 2481, 878, 966, 1693, 0, 1506, 2348, -1, 1506, -1, 1506, -1, 0, 1506, 421, 0], [0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_id(sent, ldict):\n",
    "    transformed = []\n",
    "    for w in sent.split(\" \"):\n",
    "        if w in ldict.keys():\n",
    "            transformed.append(ldict[w])\n",
    "        else:\n",
    "            transformed.append(-1)\n",
    "    return transformed\n",
    "\n",
    "lines = [ sentence_to_id(s, reduced_w2n_dict) for s in cleaned_text.split(\"<EOL>\") ]\n",
    "\n",
    "print(lines[20:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we got all sentences tokenized and words that are irrelevant marked as such. Now we can start turning these into input target pairs, while throwing every pair out that has a '-1' in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_to_samples(sentence):\n",
    "    \n",
    "    # ls stands for labeled sentence (meaning that the\n",
    "    # end of the sentence is marked with an <EOL>)\n",
    "    ls = sentence + [ reduced_w2n_dict['<EOL>'] ]\n",
    "    \n",
    "    x_vals = [ [ls[i]] for i in range(len(sentence)) ]\n",
    "    y_vals = [ ls[i+1] for i in range(len(sentence)) ]\n",
    "    \n",
    "    cleaned_x = []\n",
    "    cleaned_y = []\n",
    "    \n",
    "    for idx in range(len(x_vals)):\n",
    "        if x_vals[idx][0] != -1 and y_vals[idx] != -1:\n",
    "            cleaned_x.append(x_vals[idx])\n",
    "            cleaned_y.append(y_vals[idx])\n",
    "    \n",
    "    return cleaned_x, cleaned_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "for line in lines:\n",
    "    x, y = sent_to_samples(line)\n",
    "    inputs += x\n",
    "    labels += y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2487], [932], [1449], [903], [2239], [2199], [1275], [2207], [1695], [2733]]\n",
      "[932, 1449, 903, 2239, 2199, 1275, 680, 1695, 2733, 932]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[20:30])\n",
    "print(labels[20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 708665 out of 7082116 samples\n"
     ]
    }
   ],
   "source": [
    "# We have more than plenty of data, so we will keep a random sample of it\n",
    "keep_prob = 0.1\n",
    "\n",
    "import random\n",
    "\n",
    "training_inputs = []\n",
    "training_labels = []\n",
    "\n",
    "for idx in range(len(inputs)):\n",
    "    if random.random() < keep_prob:\n",
    "        training_inputs.append(inputs[idx])\n",
    "        training_labels.append(labels[idx])\n",
    "        \n",
    "print(\"Kept {} out of {} samples\".format(len(training_inputs), len(inputs)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to introduce the neural network\n",
    "\n",
    "Now that we got our data set up, we can cast it into a numpy array and one-hot-encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "training_inputs = np.array(training_inputs)\n",
    "training_labels = to_categorical(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set some parameters that influence how the network will perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "embedding_dim = 256\n",
    "mem_size = 256\n",
    "\n",
    "# --- Auto computed\n",
    "vocab_size = len(reduced_w2n_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 256)            714496    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2791)              717287    \n",
      "=================================================================\n",
      "Total params: 1,825,767\n",
      "Trainable params: 1,825,767\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 566932 samples, validate on 141733 samples\n",
      "Epoch 1/5\n",
      "566932/566932 [==============================] - 139s - loss: 1.9187e-04 - val_loss: 1.9237e-04\n",
      "Epoch 2/5\n",
      "566932/566932 [==============================] - 140s - loss: 1.8637e-04 - val_loss: 1.9187e-04\n",
      "Epoch 3/5\n",
      "566932/566932 [==============================] - 140s - loss: 1.8550e-04 - val_loss: 1.9134e-04\n",
      "Epoch 4/5\n",
      "566932/566932 [==============================] - 140s - loss: 1.8513e-04 - val_loss: 1.9126e-04\n",
      "Epoch 5/5\n",
      "566932/566932 [==============================] - 140s - loss: 1.8490e-04 - val_loss: 1.9125e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb19265f8d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, GRU, Embedding\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add( Embedding(vocab_size, embedding_dim, input_length=1) )\n",
    "model.add( GRU(mem_size) )\n",
    "model.add( Dense(vocab_size) )\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate), loss='mse')\n",
    "model.summary()\n",
    "\n",
    "model.fit(x = training_inputs, y = training_labels, epochs = 5, batch_size=32, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the network\n",
    "\n",
    "Let's throw some words at the network and see what it predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if -> not\n",
      "if -> __name__\n",
      "if -> self\n",
      "if -> len\n",
      "if -> isinstance\n",
      "import -> os\n",
      "import -> sys\n",
      "import -> random\n",
      "import -> time\n",
      "import -> re\n",
      "def -> __init__\n",
      "def -> main\n",
      "def -> __repr__\n",
      "def -> run\n",
      "def -> __str__\n",
      "range -> (\n",
      "range -> <DOT>\n",
      "range -> bytes\n",
      "range -> \n",
      "range -> '%s\n",
      "( -> \n",
      "( -> self\n",
      "( -> 1\n",
      "( -> x\n",
      "( -> 0\n"
     ]
    }
   ],
   "source": [
    "test_words = ['if', 'import', 'def', 'range', '(']\n",
    "top_n = 5\n",
    "\n",
    "for word in test_words:\n",
    "    test_int = reduced_w2n_dict[word]\n",
    "    \n",
    "    output = model.predict(np.array([test_int]))\n",
    "    \n",
    "    candidates = output.argsort()[0][::-1]\n",
    "    \n",
    "    for idx in range(top_n):\n",
    "        print(word, \"->\", reduced_n2w_dict[candidates[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good!\n",
    "\n",
    "Now we should save the model to reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "timestr = time.strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "filename = timestr + \"--model.h5\"\n",
    "model.save(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
